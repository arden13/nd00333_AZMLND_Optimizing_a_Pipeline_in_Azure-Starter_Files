# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This is banking data, and we are predicting some y-variable that is binary yes/no but otherwise undefined.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model according to Hyperdrive was <ERROR> and I couldn't be bothered to try and figure out which of the delicately-constructed functions caused this problem. It was a pain just to learn AutoML failed at the launch because someone called clean_data() prior to it being defined in train.py.

The best performing model according to AutoML is a LightGBM classifier using maxAbs Scaling.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

We utilized a flat dataset from a web source imported into the TabularDataset azureml class. Then a classifier was constructed and tuned in two different ways.

**What are the benefits of the parameter sampler you chose?**

It predicted the values. 

**What are the benefits of the early stopping policy you chose?**

It prevented spending too much time but for heaven's sake we didn't talk about it all in the class I had to learn about it by searching "BanditPolicy" and digging through Docs. It's incredible ANYONE would pay for this; I only got it through work.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

It chose a logistic regression model with C=1.76 and max_iter 100. I'm sure it did it in a smart way, mostly because it finished unlike the other method.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

It errored and I couldn't be bothered for one, and it functioned in the other. Classic online class of "put all the teaching material in the labs but then never debug them". SKLearn was deprecated by the time this brand new class was launched. Great.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

Presumably throwing more time at the problem would help or a full grid search. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
I did it in code.
